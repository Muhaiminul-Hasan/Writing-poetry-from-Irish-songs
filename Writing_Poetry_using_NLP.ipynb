{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM001Q3oUKn+T5Y0r7v8QD9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhaiminul-Hasan/Writing-poetry-from-Irish-songs-using-NLP/blob/main/Writing_Poetry_using_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import io\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "22oO6ihQtXim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXuXJGajsVGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a808ba7c-4d4e-4b17-a72e-76d6d2232e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you just want attention, you dont want my heart ', ' maybe you just hate the thought of me with someone new']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data = \"You just want attention, you dont want my heart \\n Maybe you just hate the thought of me with someone new\"\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(corpus[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "print(input_sequences)"
      ],
      "metadata": {
        "id": "L86D2a3TtXfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9bfe20-8183-4bbc-9e58-b9c04d3c1877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2], [1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4, 1], [1, 2, 3, 4, 1, 5], [1, 2, 3, 4, 1, 5, 3], [1, 2, 3, 4, 1, 5, 3, 6], [1, 2, 3, 4, 1, 5, 3, 6, 7], [8, 1], [8, 1, 2], [8, 1, 2, 9], [8, 1, 2, 9, 10], [8, 1, 2, 9, 10, 11], [8, 1, 2, 9, 10, 11, 12], [8, 1, 2, 9, 10, 11, 12, 13], [8, 1, 2, 9, 10, 11, 12, 13, 14], [8, 1, 2, 9, 10, 11, 12, 13, 14, 15], [8, 1, 2, 9, 10, 11, 12, 13, 14, 15, 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "max_sequence_len = 20\n",
        "print(max_sequence_len)"
      ],
      "metadata": {
        "id": "xxfNciSWw7uC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d39a147-5dd7-426c-d2b2-9192d804daa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "print(input_sequences)"
      ],
      "metadata": {
        "id": "6XIfCa1dwaKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe11dadc-07c8-4c88-9353-13990eda948b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  1  5]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  1  5  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  1  5  3  6]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  1  5  3  6  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  2  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  2  9 10]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  2  9 10 11]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  2  9 10 11 12]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  8  1  2  9 10 11 12 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  8  1  2  9 10 11 12 13 14]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  8  1  2  9 10 11 12 13 14 15]\n",
            " [ 0  0  0  0  0  0  0  0  0  8  1  2  9 10 11 12 13 14 15 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs = input_sequences[:,:-1]\n",
        "labels = input_sequences[:,-1]\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes = total_words)\n",
        "\n",
        "print(xs)\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "ogzlUQeetXZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02393534-a91f-4603-cc7a-cd8d0503640d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  1  5]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  1  5  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  1  5  3  6]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  2  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  2  9 10]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  2  9 10 11]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  8  1  2  9 10 11 12]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  8  1  2  9 10 11 12 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  8  1  2  9 10 11 12 13 14]\n",
            " [ 0  0  0  0  0  0  0  0  0  8  1  2  9 10 11 12 13 14 15]]\n",
            "[ 2  3  4  1  5  3  6  7  1  2  9 10 11 12 13 14 15 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(total_words, 240, input_length=max_sequence_len - 1),\n",
        "    Bidirectional(LSTM(150, return_sequences=True)),\n",
        "    LSTM(100),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "yYT0MPpl2QHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oNQLj1xVv_XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiYqnNSB3e7h",
        "outputId": "1d1bb182-c950-4ba8-b7e0-a9e6c66297e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 19, 240)           4080      \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 19, 300)           469200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 17)                1717      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 635397 (2.42 MB)\n",
            "Trainable params: 635397 (2.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(xs, ys, epochs = 100, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp4aja9jv_UT",
        "outputId": "cb06e405-d89e-4c28-c88c-794afa9ba287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 2.8304 - accuracy: 0.0556\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.7979 - accuracy: 0.1111\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.7607 - accuracy: 0.1667\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.7147 - accuracy: 0.1111\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.6660 - accuracy: 0.1667\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.6443 - accuracy: 0.1111\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.6291 - accuracy: 0.1667\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.5859 - accuracy: 0.1667\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.5405 - accuracy: 0.2222\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.5034 - accuracy: 0.2222\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.4590 - accuracy: 0.2222\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.3934 - accuracy: 0.1667\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.3204 - accuracy: 0.1667\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.2545 - accuracy: 0.1667\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.1692 - accuracy: 0.2222\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.0905 - accuracy: 0.2778\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.0215 - accuracy: 0.2778\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.9279 - accuracy: 0.2778\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8461 - accuracy: 0.2778\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.7470 - accuracy: 0.3889\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6638 - accuracy: 0.4444\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.5542 - accuracy: 0.4444\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.4570 - accuracy: 0.3889\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.3972 - accuracy: 0.5556\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.3189 - accuracy: 0.5000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.2082 - accuracy: 0.7222\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1359 - accuracy: 0.7222\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0780 - accuracy: 0.7222\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0258 - accuracy: 0.6667\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9547 - accuracy: 0.7222\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9114 - accuracy: 0.7222\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8406 - accuracy: 0.7222\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7886 - accuracy: 0.7778\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7525 - accuracy: 0.7222\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7094 - accuracy: 0.7778\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6725 - accuracy: 0.8333\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6388 - accuracy: 0.7778\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6023 - accuracy: 0.8889\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5615 - accuracy: 0.8889\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5393 - accuracy: 0.8889\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5115 - accuracy: 0.9444\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4898 - accuracy: 0.8889\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4652 - accuracy: 0.9444\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4811 - accuracy: 0.8333\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4662 - accuracy: 0.8889\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3913 - accuracy: 0.9444\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4066 - accuracy: 0.8889\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3560 - accuracy: 0.9444\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3691 - accuracy: 0.9444\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4004 - accuracy: 0.8333\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3034 - accuracy: 0.9444\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3485 - accuracy: 0.9444\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2887 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3357 - accuracy: 0.9444\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3004 - accuracy: 0.8889\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3456 - accuracy: 0.9444\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7005 - accuracy: 0.7778\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2540 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6121 - accuracy: 0.8333\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2273 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3530 - accuracy: 0.8889\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2845 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2416 - accuracy: 0.9444\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2191 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2177 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2163 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2153 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1778 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1966 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1709 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1580 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1583 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1576 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1433 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1353 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1308 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1266 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1226 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1185 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1139 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1091 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1049 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1017 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0989 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0962 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0931 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0899 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0870 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0847 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0827 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0809 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0790 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0771 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0751 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0732 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0714 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0697 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0680 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0664 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0649 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"You dont hate my heart\"\n",
        "next_words = 20"
      ],
      "metadata": {
        "id": "J_TOh_ll3-t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding='pre')\n",
        "  predicted = model.predict(token_list, verbose=0)\n",
        "  predicted_index = np.argmax(predicted)\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted_index:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgI3ojkT4Hxn",
        "outputId": "bb74ceb5-f30c-4228-fb39-0b29b2b677ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You dont hate my heart attention dont want heart heart heart heart new new new new new new new my new new new new new\n"
          ]
        }
      ]
    }
  ]
}